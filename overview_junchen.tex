\section{Overview of \name}
\label{sec:overview}

To address the key challenges of a multicast overlay network,
\name is built on two design choices, both of which
%To optimize the performance of bulk-data replication, \name 
%addresses the key challenges of a multicast overlay network
%by two design choices, both of which 
trade marginal costs for greater performance benefits. 
Before we describes details of \name's design, we first
highlight the intuition behind these design choices and
their cost-benefit tradeoffs 
(summarized in Table~\ref{tab:design-choices}).









%The insights obtained from \company's operational experience has inspired the design of \name, a near-optimal inter-DC multicast overlay network. This section starts with \name key design choices, highlights the design philosophy behind our choices, and provide an overview of the \name system, which builds on these design choices.


\mypara{Centralized decision-making}
\name uses a centralized controller to periodically poll 
the data delivery
status at all servers, and updates the overlay path selection.
The benefits of centralized decision-making are
two-fold. 
First, having a global view on which data each server has 
received makes it possible to explore the large set of possible 
overlay servers and circumvent various performance bottlenecks,
which would be otherwise impossible. 
Second, the centralized view enables us to 
balance the number of available sources 
cross different data units, which appears to be critical to 
achieving optimal overlay multicast performance (see 
\Section\ref{??} for more details).

%Our first design choice is the fully centralized control. For large online service providers like \company, there are considerably large number of servers and exponentially more overlay paths, so it is hard to find out the optimal one for any decentralized solutions with only local information. \name, however is able to make near-optimal overlay routing and scheduling decisions with a centralized decision-making scheme. Furthermore, the embedded controller is lightweight in terms of CPU, bandwidth consumed, and thus enjoys good scalability over WANs.


\mypara{Clean bandwidth separation}
To prevent delay caused on the latency-sensitive user data, 
\name separates background bulk data transfer from 
latency-sensitive traffic.
The \name controller continuously monitors the aggregated 
volume of each traffic category (e.g., bulk data transfers, 
latency sensitive short flows), dynamically
determines how much residual bandwidth should be allocated to
multicasting bulk data, and enforces a clean separation of 
bandwdith between bulk data transfers and latency-sensitive 
traffic.
As long as the latency-sensitive traffic does not change 
dramatically over time, 
this clean separation in bandwidth can efficiently prevent
interference on latency-sensitive traffic.

%The second design choice is dynamic bandwidth separation. To prevent delay caused on the latency-sensitive user data, \name separates background bulk data transfer from latency-sensitive traffic. Specifically, \name maintains the information of all links and monitors the aggregated traffic from all latency-sensitive data, thus can dynamically calculate the residual bandwidth that can be allocated for background bulk data transfer. This clean bandwidth separation can efficiently prevent interference on latency-sensitive traffic.


\begin{table}[t]
\centering
\begin{small}
\begin{tabular}{lll}
\textbf{Design choices} & \textbf{Benefits} & \textbf{Costs} \\ \hline
\textit{\begin{tabular}[c]{@{}l@{}}Centralized \\ decision-making\end{tabular}} & \begin{tabular}[c]{@{}l@{}}Optimal control based \\ on a global view\end{tabular} & \begin{tabular}[c]{@{}l@{}}Unable to update \\ decision in realtime\end{tabular} \\ \hline
\textit{\begin{tabular}[c]{@{}l@{}}Clean bandwidth \\ separation\end{tabular}} & \begin{tabular}[c]{@{}l@{}}Less interference with\\ latency-sensitive data\end{tabular} & \begin{tabular}[c]{@{}l@{}}Relatively lower \\ link utilization\end{tabular}
\end{tabular}
\end{small}
\caption{\name's design choices and their benefit-cost tradeoffs}
\label{tab:design-choices}
\end{table}

\mypara{Performance costs}
Yet, the benefits of these design choices do not come for free. 
On one hand, \name's centralized architecture creates an unwieldy 
decision-making process that is hard to update decisions in real time 
(due to the prohibitively large scale of the problem),
or with the most up-to-date global view (due to inherent latency
to gather data from globally distributed servers).
On the other hand, simply enforcing a split on bandwidth resource
could result in low link utilization, if such split is updated on a
coarser timescale than how often traffic demands fluctuate 
significantly.

%The above two design choices may introduce performance costs, i.e., fully centralized control makes \name unable to update decisions in real time, because the \name controller works in a centralized manner and has to collect information from geo-distributed DCs, and this will naturally introduce some communication latency to the decision making process. Besides, the clean separation on bandwidth will possibly result in low link utilization due to the ever changing link utilization and the coarse-grained scheduling decisions.


\mypara{Design philosophy}
Despite the costs introduced by \name's design choices, 
we argue that the costs are favorably outweighed by the benefits.
The our intuition behind this argument is driven by several 
observations. 
\begin{enumerate}
\item Bulk data transfers happen on much longer timescales than 
how fast new transfer jobs start, and thus it can tolerates a delay
at coarse timescale of several seconds in exchange for near-optimal 
scheduling decisions. 
\item The aggregation of latency-sensitive data tend to be stable 
on timescales of several seconds, so applying a bandwidth separation 
that is update on timescales of seconds, can prevent interference on 
the latency-sensitive data while maintaining a relatively 
high link utilization. 
\item The resulting system is amenable to a simpler implementation. 
For instance, the control logic running locally in each server
is only triggered on arrivals of new data units or control messages, 
and thus can be stateless and lightweight.
\end{enumerate}

%Fortunately, the above costs are outweighed by the benefits. (1) bulk data transfer usually takes tens of seconds to minutes, so it can tolerates a delay at coarse timescale of several seconds in exchange for near-optimal scheduling decisions. (2) the aggregation of latency-sensitive data is stable on timescales of several seconds, therefore, it is plausible for the dynamic bandwidth separation to prevent interference on the latency-sensitive data while still maintaining high link utilizations. (3) the resulting system is amenable to a simpler implementation because the decision-making logic running on the controller side does not need to maintain data status or to deliver complex control messages, thus can be stateless and lightweight.

\mypara{Technical challenge}
The key technical challenge here is how to make optimal overlay scheduling and routing decisions at the scale tens of thousands of objects and tens of thousands of servers in near real time. To achieve desirable performance in a multicast overlay network, fully exploiting all the available overlay paths is essential, but it is untenable to go through all the potential servers and exponentially more paths by traditional approaches.

%\begin{itemize}
%
%\item Idea \#1: Fully centralized control
%
%\item Idea \#2: Dynamic bandwidth separation: separating background bulk data transfer from latency-sensitive traffic
%
%\item These ideas introduces performance costs (not real time, potentially low link utilization) are outweighed by benefits (not real time, potentially low link utilization)
%
%\item Design philosophy: the costs are outweighed by the benefits. (1) bulk data transfer can tolerate updates at coarse timescales. (2) the aggregation of latency-sensitive data is stable on timescales of several seconds. (3) the resulting system is amenable to simpler implementation.
%
%\item Key technical challenge: how to make optimal overlay scheduling and routing decisions at the scale tens of thousands of objects and tens of thousands of servers in near real time.
%
%\end{itemize}



