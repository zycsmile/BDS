\section{System Design}
\label{sec:system}

This section presents the detailed system design of \name.% to realize the two design choices presented in \Section\ref{sec:overview}.

\subsection{\name architecture}
\label{subsec:system:architecture}

\begin{figure}[t]
  \centering
  \includegraphics[width=3in]{images/implementation_v2.eps}
  \tightcaption{The implementation of \name.}
  \label{fig:implementation}
\vspace{-0.4cm}
\end{figure}
%\vspace{-15pt}

\name takes advantages of the underlying multicast system and the detailed architecture is shown in Figure \ref{fig:implementation}. It consists of four components: Controller, Agent Monitor, Agent and Network Monitor. (1) \emph{Controller} is a scheduler which executes the customized scheduling and routing algorithm. (2) \emph{Agent Monitor} delivers messages between \emph{Controller} and \emph{Agents}. (3) \emph{Agent} is a module deployed in each node. It announces the bulk data download requirements to the agent monitor and handles specific transmissions on local nodes. (4) \emph{Network Monitor} monitors the latency-sensitive traffic and link utilization, which are the basic inputs of the centralized algorithm.

\subsection{Centralized control}
\label{subsec:system:centralized}

The centralized controller of \name sets a 3-second scheduling cycle and updates the scheduling results in each cycle. The control level interfaces are implemented by HTTP POST (including: 1. the data collection from agents to the controller, and 2. the control messages updating from the controller to agents). Specifically, the basic workflow in one cycle can be described as follows: (1) the local agent on each node checks the block status, records the IDs of blocks that finished downloading in this cycle, and then reports the status and information to the agent monitor. (2) Agent Monitor aggregates the information from all the agents, updates block duplication status and server availability status, and then sends the updated information to the controller. (3) the controller runs the centralized scheduling algorithm, works out the near-optimal scheduling and routing results, and sends them to the agent monitor. (4) agent monitor then forwards the control messages back to the corresponding local agents. (5) the local agent sets the transmission parameters according to the received control message and uses \emph{wget} tools to download the bulk data.
%\jc{the para needs a clearer message: how centralized control is implemented? what's the data collection interface? what's the control interface? Don't use mathmatical notations (again don't assume people have read section 4) or protocols like http post (they belong to implementation).}

There are two further optimizations in centralized control implementation.
\begin{itemize}
\item \emph{Blocks merging}. \name merges the blocks with the same source and destination pair into one subtask. There are two advantages: reducing the computation scale and achieving higher-efficient transmissions. To be specific: 1) there will be a large number of pending blocks in each scheduling cycle, making calculation on the controller computationally hard to finish within an acceptable time, while block merging can significantly reduce the block number and thus reduce the calculation scale; 2) concurrent transmission will make all blocks establish connections at the same time, sharing the limited bandwidth and thus cannot possibly be finished in one cycle. This will result in connection hanging and inefficient transmissions.
\item \emph{Non-blocking update}. No matter how fast the centralized algorithm runs, it still takes some time to work out the scheduling and routing solutions in each cycle. During this algorithm running time, \name continues the transmissions according to the configurations in the last cycle instead of pausing them and waiting for new configurations. To ensure consistency, the controller will estimate the task status at algorithm ending time and use that future status as inputs of the scheduling algorithm.
\end{itemize}

%\begin{itemize}
%
%\item Start with the basic workflow of each 3-second cycle: (1) how local agent collects delivery status, (2) send messages to the controller, (3) controller runs the algorithm, (4) control message to each local agent, and (5) how local agent enforce decision.
%
%\item Fault tolerance: what if a server is not available (or straggling), what if one controller instance is not available, what if there is network partition between DCs or between DCs and the controller.
%
%\item Explain two optimizations:
%\begin{itemize}
%\item Merging blocks
%\item Non-blocking update
%\end{itemize}
%
%\end{itemize}

\subsection{Dynamic bandwidth separation}
\label{subsec:system:separation}

To guarantee dynamic bandwidth separation between bulk data and delay-sensitive user data, \name monitors all the latency-sensitive traffic classified by source IPs and aggregates traffic of all the inter-/intra-DC links in real time.

With the known link capacity and the aggregated size of latency-sensitive traffic, the residual bandwidth for background data can then be calculated by the difference. So the upper bound of the available bandwidth for bulk-data transfer can then be confirmed. In the server end, each agent enforces the bandwidth limits by Linux Traffic Control (TC). Thus, \name achieves the bandwidth separation dynamically and in real-time.

Such dynamic bandwidth separation scheme is different from the traditional priority-based techniques (such as \cite{kumar2015bwe}), which sets higher priority to online latency-sensitive traffic than background traffic. When traffic bursts, such priority-based techniques work as usual and allocate bandwidth according to traffic priorities, no matter whether the online traffic is suffering from high latency. As such situation can be avoided in \name, which dynamically monitors the aggregated bandwidth occupied by delay-sensitive applications and reserves enough bandwidth for them, \name guarantees the performance of latency-sensitive applications by leveraging dynamic bandwidth separation.


%\jc{please put this solution into context. what's the diff to priority-based techniques such as bwe?}


\subsection{Fault tolerance}
\label{subsec:system:fault}
To make \name fault tolerant, the following scenarios are considered and the corresponding evaluations are in \Section\ref{subsubsec:evaluation:adaptability}.

\begin{packedenumerate}
\item \emph{What if the controller is unavailable?} The controller is replicated three times for fault recovery. Once the master controller fails, one replica will be brought up using distributed consensus protocols such as \cite{lamport1998part}.
\item \emph{What if a server is unavailable?} If the agent on this server is still able to work, it will report the abnormal state to the agent monitor. Otherwise, those servers that selected this server as data source would report the unavailability to the agent monitor. In both cases, the controller would remove that server from the potential data sources in the next scheduling cycle.
\item \emph{What if there is network partition between DCs or between DCs and the controller?} If network partition happens between DCs and the controller, the whole network will fall back to a default decentralized overlay protocol to ensure graceful performance degradation. If network partition happens between DCs, the DCs located in the same partition with the controller will work the same as before, while the separated DCs will fall back to the decentralized overlay network.
\end{packedenumerate}
%
%\begin{itemize}
%
%\item First, how to get real-time aggregated size of latency-sensitive traffic.
%
%\item Second, how to calculate the bandwidth cap for background bulk traffic
%
%\item Finally, how to enforce the bandwidth cap.
%
%\end{itemize}



