\section{Introduction}

Global-scale online services, such as Google, Facebook, and 
Baidu, transfer massive amounts of data between geo-distributed 
datacenters (DCs). 
A common communication pattern is {\em bulk-data multicast}, 
which replicates data 
(typically several to 100s GB\jc{please check}), such as user 
logs, search engine indexes, and databases\jc{anything else?}, 
to any set of destination DCs. 
As data sizes continue to explode and more DCs are deployed to 
reach a global footprint, how to multicast bulk data in a timely 
and cost-effective manner has been a key driving force behind 
recent efforts to improve the performance of inter-DC 
WANs~\cite{b4,bwe,swan,??,??,??}.


%Global-scale online services, such as Google, Facebook, and Baidu, 
%depend crucially on the ability to distribute data across 
%geo-distributed datacenters (DCs) in a timely and cost-effective 
%manner. This has introduced substantial challenges, as data sizes 
%continue to explode and more DCs are deployed to reach a global 
%footprint. 
%These trends have been a key driving force behind recent efforts 
%to optimize the WAN performance between two 
%DCs~\cite{b4,bwe,swan,??,??,??}. 

In this paper, we argue that the existing approaches that focus 
on WAN performance are fundamentally insufficient to optimize 
bulk-data multicast, because they fall 
short of harnessing servers' capability to store-and-forward 
data, and exploring the abundant disjoint paths via servers 
in different DCs. 
For instance, the multicast throughput could potentially be 
improved by splitting data into fine-grained units and 
delivering them simultaneously via multiple servers selectively 
picked as intermediate points to circumvent slow paths in DC
networks and WANs.


Inspired by the success of early work on multicast overlay 
networks, we argue that optimize bulk-data multicast performance,
an similar {\em application-level multicast overlay network} 
is needed to optimally deliver data in
a way that fully utilizes the overlay paths that are in abundance 
between DCs.
To replicate a data file from one DC to multiple DCs, 
the overlay multicast network should split the data into small 
units, send them simultaneously along selectively picked overlay 
paths, and dynamically adapt the overlay routing in response to 
changes in network conditions and 
resource availability.


While multicast overlays are well studied in several settings
(e.g.,~\cite{??,??,??,??}), introducing multicast overlay networks 
to large online  service providers, 
like Google and Baidu, poses new challenges.
%Building on the operational experience of 
%\company\footnote{Anonymized for double-blind reviewing}, 
%a large online service providers, we summarize two challenges,
%which to our best knowledge do not have direct solution.
First, the sheer numbers of servers and overlay paths render 
traditional approaches ineffective: it is untenable 
to track data delivery status at all servers and update 
decisions in real time, nor is it effective to fully the exploit 
overlay paths by using decentralized protocols
(e.g.,~\cite{??,??}), 
or enforcing a strictly structured logical topology to reduce 
the number of available overlay paths (e.g.,~\cite{akamai}).
Second, because any small increase in the delay of
latency-sensitive traffic can cause substantial revenue losses, 
there is a strong need to prevent any delay on
latency-sensitive traffic, even at the expense of a 
slightly lower link utilization.

%we argue that an {\em inter-DC multicast 
%overlay} network that optimally schedules and routes data via 
%overlay paths is essential to ensuring the desirable performance 
%for two reasons.
%First, the need for multicasting data to a subset of (or all) DCs 
%is endemic in online service providers; e.g.,
%Google has seen a \fillme-fold increase in \fillme years
%in the amount of data that needs to be distributed to all 
%DCs~\cite{??}, and this number would only multiply with more 
%DCs deployed.
%Second, the improved WAN performance between any DC pair can be
%fully utilized only when used with an inter-DC multicast overlay 
%network that 
%splits data in fine granularities and optimally select 
%overlay paths to circumvent inter-/intra-DC bottlenecks.



%%While these efforts have shown promising improvement on the performance of pairwise 
%%inter-DC WANs, we argue that an efficient multicast overlay network that optimally 
%%schedules and routes data from each DC to multiple DCs via inter-DC overlay paths 
%%is essential to ensuring desirable performance and fully utilizing the improved 
%%pairwise inter-DC data transfer.
%While multicast overlay networks have been studied extensively 
%in settings of peer-to-peer (P2P) streaming~\cite{??,??} and 
%content delivery networks (CDNs)~\cite{??,??}, it remains unclear 
%whether existing approaches apply to the scale of large online 
%service providers like Google and Baidu. 
%%The conventional wisdom has been that to operate at scale and react 
%%in real time, one has to either organize overlay nodes in a 
%%strictly structured (and thus suboptimal) topology 
%%(e.g.,~\cite{akamai}), or use a hybrid (and thus more 
%%complex) control mechanism (e.g.,~\cite{vdn}) with a local 
%%logic adapting in real time and a global logic updating every
%%few minutes.
%Drawing on the operational experience from 
%\company\footnote{Anonymized for double-blind reviewing}, a large 
%online service providers, we see two key requirements
%of an inter-DC multicast overlay network.
%First, because these DCs have more servers and thus 
%exponentially more overlay paths than CDNs or P2Ps, 
%it is untenable to exploit all overlay paths by 
%traditional approaches, such as decentralized or hybrid path 
%selection (e.g.,~\cite{??,??}), or structured topologies 
%(e.g.,~\cite{akamai}).
%Second, these WANs are shared by latency-sensitive traffic 
%and bulk background traffic. 
%Because any small increase in delay of
%latency-sensitive traffic can cause substantial revenue losses, 
%there is a strong need to prevent interference on 
%latency-sensitive traffic, even at the expense of slightly lower 
%link utilization.


%while a more decentralized overlay network protocol 
%is conceptually simpler, it is untenable for them to fully 
%utilize the overlay paths, so we see a great room for 
%improvement, especially at tail performance. This motivates 
%the fully centralized architecture that maintains a fresh 
%global view for decision making.
%Second, adding even a small delay in latency-sensitive data 
%can cause significant revenue losses. Therefore, it is 
%acceptable to enforce a clean bandwidth separation between 
%bulk traffic delivery and latency-sensitive data, even at 
%the cost of lower link utilization.
%
%But designing a multicast overlay for DCs of online service 
%providers is different in two key aspects.
%First, these DCs have more servers in DCs, and much more overlay 
%paths, so it is necessary to maintain an up-to-date global view 
%of data delivery status on all servers to fully exploit these 
%paths.
%Second, the WANs are shared by latency-sensitive user data and 
%background bulk data, creating a need for minimizing the data 
%distribution latency while preventing negative 
%impacts on the latency-sensitive data. 

This paper presents {\em \name}, a near-optimal multicast 
overlay network for inter-DC bulk data multicast.
\name dynamically splits, reorders, and delivers data via 
overlay paths between the source DC and destination DCs.
%This paper presents {\em \name}, a near-optimal inter-DC multicast 
%overlay network, which minimizes data distribution delay from one 
%DC to any subset DCs by dynamically splitting, 
%reordering, and deliver data via overlay paths selected 
%from all server-level paths between the source DC and destination DCs.
%\name focuses on distributing background bulk data (e.g., \fillme), 
%which is by several orders of magnitudes larger than 
%latency-sensitive user data.
At the core of \name are two design choices.
\begin{packeditemize} 
\item {\em Centralized decision-making:} 
Decision making in \name is fully centralized; the \name 
controller maintains a global view of data delivery status on 
all servers 
and makes overlay routing and scheduling decisions every 
few seconds (by default, 3 seconds).
\name solves a multicommodity problem using 
a near-optimal algorithm amenable to efficient incremental 
updates.
\item {\em Dynamic bandwidth separation:}
To prevent delay caused by background data on 
latency-sensitive user data, \name monitors the 
aggregated traffic volume 
of latency-sensitive data, and maintains a dynamic bandwidth 
separation by enforcing the maximum amount of bulk-data 
multicast sent from each server. 
\end{packeditemize}

While \name's design choices introduce performance costs (i.e.,
\name does not update decisions in real time or achieve full 
link utilization), our design philosophy is that these costs are 
favorably outweighed by several benefits.
% of near-optimal global optimization 
%as well as the resulting simpler solution from an engineering
%perspective.
%This observation inspired the key insight underlying \name that 
%multicasting bulk data can tolerate a small amount of delay in 
%exchange for closer-to-optimal decisions overlay routing and 
%scheduling.
(1) Since delivering bulk data takes tens of seconds to 
minutes, \name can tolerate a delay of updating centralized 
control decisions at coarse timescales of several 
seconds in exchange for closer-to-optimal decisions made by  
the centralized controller.
(2) The observation that the aggregation of 
latency-sensitive traffic tends to 
be stable on timescales of several seconds suggests that it is 
plausible to eliminate undesired interference on 
latency-sensitive traffic by clean bandwidth separation 
while achieving a high link utilization.
%Moreover, having a global view on the delivery status of all objects in all 
%servers allows the controller to remarkably improve overlay routing and 
%scheduling decisions, which would be impossible otherwise.
(3) Finally, these design choices are amenable to a simple
implementation from an engineering perspective;
e.g., the program running locally on the server side is 
only triggered by data arrivals or control messages, and 
thus can be largely stateless.


We implemented a prototype of \name and integrated it in 
\company\footnote{Anonymized for double-blind review}, 
one of the largest search engine service
providers. We deployed \name in ten of \company's DCs, and 
ran pilot study on \fillme PB data over \fillme days. Our
real-world experiments show that \name achieves 3-5$\times$ 
speedup over \company's existing solution. 
Using real trace-driven evaluation and microbanchmarking, 
we also show that \name outperforms many techniques used in 
today's CDNs, and \name can scale out to the traffic volume 
of \company's inter-DC WAN, and tolerate various failure 
scenarios.
