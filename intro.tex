\section{Introduction}

Global-scale online services, such as Google, Facebook, and
Baidu, transfer massive amounts of data between geo-distributed
datacenters (DCs).
One of the most common communication patterns of inter-DC workloads
is {\em bulk-data inter-DC multicast},
which replicates data (e.g., user
logs, search engine indexes, offline file sharing, forum posts, and
databases) from one DC to any set of destination DCs.
Our study on inter-DC workload of \company\footnote{Anonymized for
double-blind review}, a large search engine provider, reveals that
91\% of inter-DC traffic is bulk-data multicasts
(\Section\ref{sec:motivation}).
As data sizes continue to explode and more DCs are deployed to
reach a global footprint, how to multicast bulk data in a timely
and cost-effective manner has been a key driving force behind
recent efforts to improve the performance of inter-DC
WANs~\cite{savage1999Theend,jain2013b4,kumar2015bwe,hong2013achieving,zhang2015guarantee}.


%Global-scale online services, such as Google, Facebook, and Baidu,
%depend crucially on the ability to distribute data across
%geo-distributed datacenters (DCs) in a timely and cost-effective
%manner. This has introduced substantial challenges, as data sizes
%continue to explode and more DCs are deployed to reach a global
%footprint.
%These trends have been a key driving force behind recent efforts
%to optimize the WAN performance between two
%DCs~\cite{b4,bwe,swan,??,??,??}.

In this paper, we argue that the existing approaches which focus
on improving WAN performance have fundamentally limitations,
since they fall short of fully exploring the abundant
disjoint inter-DC paths, and fail to harness servers'
capability to store-and-forward data.
Instead, the multicast throughput could be substantially
improved by splitting data into fine-grained units and delivering
them simultaneously via servers selectively picked as
intermediate points to circumvent slow WAN paths and performance
bottlenecks in DC networks.


Inspired by the success of multicast overlay
networks, we argue that to optimize bulk-data multicast,
a similar {\em application-level multicast
overlay network} is needed to optimally deliver data in a way that
fully utilizes inter-DC overlay paths.
To replicate a data file from one DC to multiple DCs, this overlay
multicast network will send different parts of the data
simultaneously along selectively picked overlay paths, and
dynamically adapt the overlay routing in response to changes in
network conditions and resource availability.


While multicast overlays are well studied in several other settings
(e.g.,~\cite{Liebeherr2002Application,Wang2007mTreebone,
Andreev2013Designing,Mokhtarian2015Minimum}), introducing multicast overlay networks
to large online  service providers,
like Google and Baidu, poses new challenges.
%Building on the operational experience of
%\company\footnote{Anonymized for double-blind reviewing},
%a large online service providers, we summarize two challenges,
%which to our best knowledge do not have direct solution.
First, the sheer numbers of servers and overlay paths makes
it untenable to track data delivery status at all servers and
update decisions in real time.
Nor is it effective to fully the exploit
overlay paths by each server making decisions locally
(e.g.,~\cite{kostic2003bullet,Repantis2010Scaling}),
or reducing the number of available overlay paths by a
strictly structured logical topology
(e.g.,~\cite{Nygren2010The}).
Second, because any small increase in the delay of
latency-sensitive traffic due to competing bulk-data transfers
 can cause unbearable revenue losses,
there is a strong need to prevent any delay on
latency-sensitive traffic.
%, even at the expense of a slightly lower link utilization.

%we argue that an {\em inter-DC multicast
%overlay} network that optimally schedules and routes data via
%overlay paths is essential to ensuring the desirable performance
%for two reasons.
%First, the need for multicasting data to a subset of (or all) DCs
%is endemic in online service providers; e.g.,
%Google has seen a \fillme-fold increase in \fillme years
%in the amount of data that needs to be distributed to all
%DCs~\cite{??}, and this number would only multiply with more
%DCs deployed.
%Second, the improved WAN performance between any DC pair can be
%fully utilized only when used with an inter-DC multicast overlay
%network that
%splits data in fine granularities and optimally select
%overlay paths to circumvent inter-/intra-DC bottlenecks.



%%While these efforts have shown promising improvement on the performance of pairwise
%%inter-DC WANs, we argue that an efficient multicast overlay network that optimally
%%schedules and routes data from each DC to multiple DCs via inter-DC overlay paths
%%is essential to ensuring desirable performance and fully utilizing the improved
%%pairwise inter-DC data transfer.
%While multicast overlay networks have been studied extensively
%in settings of peer-to-peer (P2P) streaming~\cite{??,??} and
%content delivery networks (CDNs)~\cite{??,??}, it remains unclear
%whether existing approaches apply to the scale of large online
%service providers like Google and Baidu.
%%The conventional wisdom has been that to operate at scale and react
%%in real time, one has to either organize overlay nodes in a
%%strictly structured (and thus suboptimal) topology
%%(e.g.,~\cite{akamai}), or use a hybrid (and thus more
%%complex) control mechanism (e.g.,~\cite{vdn}) with a local
%%logic adapting in real time and a global logic updating every
%%few minutes.
%Drawing on the operational experience from
%\company\footnote{Anonymized for double-blind reviewing}, a large
%online service providers, we see two key requirements
%of an inter-DC multicast overlay network.
%First, because these DCs have more servers and thus
%exponentially more overlay paths than CDNs or P2Ps,
%it is untenable to exploit all overlay paths by
%traditional approaches, such as decentralized or hybrid path
%selection (e.g.,~\cite{??,??}), or structured topologies
%(e.g.,~\cite{akamai}).
%Second, these WANs are shared by latency-sensitive traffic
%and bulk background traffic.
%Because any small increase in delay of
%latency-sensitive traffic can cause substantial revenue losses,
%there is a strong need to prevent interference on
%latency-sensitive traffic, even at the expense of slightly lower
%link utilization.


%while a more decentralized overlay network protocol
%is conceptually simpler, it is untenable for them to fully
%utilize the overlay paths, so we see a great room for
%improvement, especially at tail performance. This motivates
%the fully centralized architecture that maintains a fresh
%global view for decision making.
%Second, adding even a small delay in latency-sensitive data
%can cause significant revenue losses. Therefore, it is
%acceptable to enforce a clean bandwidth separation between
%bulk traffic delivery and latency-sensitive data, even at
%the cost of lower link utilization.
%
%But designing a multicast overlay for DCs of online service
%providers is different in two key aspects.
%First, these DCs have more servers in DCs, and much more overlay
%paths, so it is necessary to maintain an up-to-date global view
%of data delivery status on all servers to fully exploit these
%paths.
%Second, the WANs are shared by latency-sensitive user data and
%background bulk data, creating a need for minimizing the data
%distribution latency while preventing negative
%impacts on the latency-sensitive data.

This paper presents {\em \name}, a near-optimal multicast
overlay network for inter-DC bulk data multicast.
At the core of \name is the decision of {\em fully centralizing
the scheduling and routing of bulk-data multicast}.
%This paper presents {\em \name}, a near-optimal inter-DC multicast
%overlay network, which minimizes data distribution delay from one
%DC to any subset DCs by dynamically splitting,
%reordering, and deliver data via overlay paths selected
%from all server-level paths between the source DC and destination DCs.
%\name focuses on distributing background bulk data (e.g., \fillme),
%which is by several orders of magnitudes larger than
%latency-sensitive user data.
Contrary to the intuition that, in order to scale out
and be responsive, servers must retain the capability to make
certain local decisions, \name's centralized design
is built on the several empirical observations:
\begin{packeditemize}
\item First, while it is indeed impractical to update centralized
decision-making in real-time, bulk-data transfers, which take
at least tens of seconds, can tolerate a small update delay in
overlay routing decisions
in the hope of getting closer-to-optimal decisions based on
a global view of data delivery status.
\item Second, centrally coordinated decision-making makes it easier
to enforce bandwidth allocation on all overlay paths,
and thus eliminate interference of bulk-data
transfers on latency-sensitive traffic.
\item Finally, a centralized design is amenable to a simple
implementation from an engineering perspective;
e.g., the program running locally on the server side is
only triggered by data arrivals or control messages, and
thus can be largely stateless.
\end{packeditemize}


The key challenge for \name to realize these benefits is how to
update the centralized decision-making in a near-optimal and
efficient manner.
\name addresses this challenge by a novel technique that
{\em decouples} the centralized decision-making process into a
subprocess of scheduling data transfers and a subprocess of
optimally routing each data transfer along overlay paths.
Our technique achieves performance with proven gap to that of
optimal decisions, and can be updated in several seconds at the
scale of typical traffic demand of a large online service
provider (10s of DCs, each with 1000s of servers).


%At the core of \name are two design choices.
%\begin{packeditemize}
%\item {\em Centralized decision-making:}
%Decision making in \name is fully centralized; the \name
%controller maintains a global view of data delivery status on
%all servers
%and makes overlay routing and scheduling decisions every
%few seconds (by default, 3 seconds).
%\name solves a multicommodity problem using
%a near-optimal algorithm amenable to efficient incremental
%updates.
%\item {\em Clean bandwidth separation:}
%To prevent delay caused by background data on
%latency-sensitive user data, \name monitors the
%aggregated traffic volume
%of latency-sensitive data, and maintains a clean, dynamic
%bandwidth
%separation by enforcing the maximum amount of bulk-data
%multicast sent from each server.
%\end{packeditemize}

%While \name's design choices introduce performance costs (i.e.,
%\name does not update decisions in real time or achieve full
%link utilization), our design philosophy is that these costs are
%favorably outweighed by several benefits.
%% of near-optimal global optimization
%%as well as the resulting simpler solution from an engineering
%%perspective.
%%This observation inspired the key insight underlying \name that
%%multicasting bulk data can tolerate a small amount of delay in
%%exchange for closer-to-optimal decisions overlay routing and
%%scheduling.
%(1) Since delivering bulk data takes tens of seconds to
%minutes, \name can tolerate a delay of updating centralized
%control decisions at coarse timescales of several
%seconds in exchange for closer-to-optimal decisions made by
%the centralized controller.
%(2) The observation that the aggregation of
%latency-sensitive traffic tends to
%be stable on timescales of several seconds suggests that it is
%plausible to eliminate undesired interference on
%latency-sensitive traffic by clean bandwidth separation
%while achieving a high link utilization.
%%Moreover, having a global view on the delivery status of all objects in all
%%servers allows the controller to remarkably improve overlay routing and
%%scheduling decisions, which would be impossible otherwise.
%(3) Finally, these design choices are amenable to a simple
%implementation from an engineering perspective;
%e.g., the program running locally on the server side is
%only triggered by data arrivals or control messages, and
%thus can be largely stateless.


We implemented a prototype of \name and integrated it in
\company,
one of the largest search engine service
providers. We deployed \name in ten of \company's DCs, and
ran pilot study on 500 TB data over 7 days. Our
real-world experiments show that \name achieves 3-5$\times$
speedup over \company's existing solution.
Using real trace-driven evaluation and microbanchmarking,
we also show that \name outperforms many techniques used in
today's CDNs, and \name can scale out to the traffic volume
of \company's inter-DC WAN, and tolerate various failure
scenarios.


Our key contributions can be summarized as following:
\begin{packeditemize}
\item Characterizing the workload of inter-DC multicasts of bulk data
to motivate the benefits and challenges of inter-DC multicast overlay
networks (\Section\ref{sec:motivation}).
\item Designing \name, which achieves near-optimal flow completion
time with a centralized control scheme (\Section\ref{sec:overview},\ref{sec:logic},\ref{sec:system}).
\item Demonstrating the practical benefits of \name by a real-world
 deployment in a large online service provider (\Section\ref{sec:evaluation}).
\end{packeditemize}
