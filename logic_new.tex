\section{Near-Optimal and Efficient Decision-Making Logic}
\label{sec:logic}

At the core of \name is a centralized decision-making
algorithm that periodically updates near-optimal overlay
routing decisions at scale  in near real-time.
% (e.g., $10^4$s of overlay paths and $10^5$s of data blocks).
%At a high level, to optimize inter-DC data multicast, \name fully
%exploits application-level overlay paths by splitting data into
%small blocks and periodically selecting the overlay paths to send
%each data block.
%While achieving this may be indeed intractable due to the
%sheer numbers of available overlay paths and data blocks,
\name strikes a favorable tradeoff between solution optimality
and fast update by
{\em decoupling} the problem into two steps (\Section\ref{subsec:logic:separation}):
overlay scheduling, i.e., which data blocks to be sent
(\Section\ref{subsec:logic:scheduling});
and routing, i.e., which path to send each data block
(\Section\ref{subsec:logic:routing}), each of which
can be solved efficiently and near-optimally with proved guarantees
(\Section\ref{subsec:logic:scheduling},~\ref{subsec:logic:routing}).

%At a high level, \name optimizes the data distribution performance by splitting data into fine-grained blocks so as to exploiting all available server-level overlay paths, and possible reordering of blocks to speed up the process.
%In a general case, it is indeed intractable to solve the problem in near real-time, but \name can find a near-optimal solution for our problem scale in several seconds by using applying two approximations: (1) separating the problem of data scheduling and overlay routing, and (2) using standard linear-programming relaxation to solve them efficiently.

%\jc{In general, please avoid use of big formulations (like eq 5-10 on p6). May strike a negative impression in nsdi submissions}

\subsection{Basic formulation of overlay control}
\label{subsec:logic:formulation}

%\begin{table}[t]
%\begin{center}
%%\resizebox{\textwidth/2}{!}{
%%\begin{tabular}{p{2cm}<{\centering}|p{2cm}<{\centering}}
%\begin{tabular}{| c | l|}
%\hline
% \rowcolor[gray]{0.9}
%\textbf{Variables} & \textbf{Meaning} \\
%\hline \hline
%\textit{$\mathbb{A}$} & Set of all $(s, d)$ pair\\
%\hline
%\textit{$\mathbb{B}$} & Set of blocks of all tasks\\
%\hline
%\textit{$B_{i,j}$} & Block $i$ in Task $j$\\
%\hline
%\textit{$c(l_{u,v})$} & Capacity of link $l_{u,v}$\\
%\hline
%\textit{$Path(s,d)$} & Set of all potential paths in $\mathbb{A}$\\
%\hline
%\textit{$f_{B_{i,j},p_\lambda}$} & Allocated bandwidth for $B_{i,j}$ on path $p_\lambda$\\
%\hline
%\textit{$I_{B_{i,j},p_\lambda}$} & 0 or 1: whether $p_\lambda$ is selected for $B_{i,j}$\\
%\hline
%\end{tabular}
%%}
%\end{center}
%\caption{Variables in \name.}
%\label{table:para}
%\end{table}

\begin{table}[t]
\begin{center}
\resizebox{3in}{!}{
%\begin{tabular}{p{2cm}<{\centering}|p{2cm}<{\centering}}
\begin{tabular}{| c | l|}
\hline
 \rowcolor[gray]{0.9}
\textbf{Variables} & \textbf{Meaning} \\
\hline \hline
\textit{$\mathbb{A}$} & Set of all source and destination (s,d) pairs\\
\hline
\textit{$\mathbb{B}$} & Set of blocks of all tasks\\
\hline
\textit{$B_{i,j}$} & Block $i$ in Task $j$\\
\hline
\textit{$c(l_{u,v})$} & Capacity of link $l_{u,v}$\\
\hline
\textit{$R_{up}(n)/R_{down}(n)$} & Upload/download rate limit of server $n$\\
\hline
\textit{$Path(s,d)$} & Set of all possible paths in $\mathbb{A}$\\
\hline
\textit{$s_{B_{i,j}}$} & Data source of $B_{i,j}$\\
\hline
\textit{$f_{B_{i,j},p_\lambda}$} & Transmission rate of $B_{i,j}$ on path $p_\lambda$\\
\hline
\textit{$I_{B_{i,j},p_\lambda}$} & 0 or 1: whether $p_\lambda$ is selected for $B_{i,j}$\\
\hline
%\textit{$f(l_{u,v}$)} & The allocated transmission rate on link $l_{u,v}$\\
%\hline
\end{tabular}
}
\end{center}
\vspace{-0.4cm}
\tightcaption{Notations in \name.}
\label{table:para}
\end{table}

\mypara{Under fixed network capacity}
We start with the basic formulation of the overlay control under the assumption of fixed network capacity and fixed demand of multicast requests. We will later extend it to dynamic network performance and traffic demands. With the key variables and parameters summarized in the Table \ref{table:para}, the problem of multicast overlay routing can be formulated as follows:
%The data distribution problem over inter-DC WANs is defined as follows: We are given one data source DC $S$ and sets of destinations DC $D=\{d_i\}$ in an overlay network, what's the optimal transmission strategy with shortest completion time under a serious of constraints. To formulate this problem and design the decision-making logic for the centralized controller, we should first clarify the following aspects (Table \ref{table:para} summarizes some key variables used in \name):

%\jc{Notions are too complicated. Please simplify. 4.1 should be at most 1/3 pg.}

\noindent(1) {\em Input.}% \jc{the formulation only consider one multicast request? No, multiple.}
%One source DC, sets of destination DCs,
%network topology,
%server upload (and download) rate limit,
%link capacity.
Each multicast transmission, referred to as a {\em task} $T$, is defined
by a source DC, a set of destination DCs, and a data file chopped into
a list of fixed-sized data
{\em blocks}\footnote{Splitting data into fine-grained blocks enables
parallel transfer along
multiple overlay paths to increase throughput, though too small data blocks
increases computational overhead of the routing algorithm.
In practice, we found 2MB is a good tradeoff
(\Section\ref{subsec:evaluation:benchmarks:parameters})}.
%the block size is 2MB, for efficient transmission and acceptable calculation overhead)
The $i$th block of the $j$th task is denoted as $B_{i,j}$.
Besides traffic demand, the inputs also include link capacity $c(l_{u,v})$ and
server upload (and download) rate limit $R_{up}(n)/R_{down}(n)$. %\jc{what're the notions of toplogy, bandwidth? aren't link capacity and rate limit the same? Not the same, link capacity is the attribute of links and rate limit is the attribute of servers.}

\noindent(2) {\em Output.} A four tuple for each server: $\langle \overrightarrow{o_\mathbb{B}}, s_{\mathbb{B}}^*, p_{\lambda}^*, f^*_{\mathbb{B},p_{\lambda}^*} \rangle$, which denotes the block transmission sequence, the optimal source, overlay path for each block, and the optimal allocated bandwidth on this path, respectively. %\jc{what the hell is B?!, and where's the server notation?}

\noindent(3) {\em Constraints.}
The formulation is subject to constraints similar to those of maximum concurrent flow (MCF) problem~\cite{garg2007faster,reed2012traffic}.\jc{you can tighten the space between equations and texts, if you need space}
%There are four constraints on link capacity, data size, bandwidth allocation and path selection:

%The mentioned three constraints can then be formulated as follows:
\begin{packeditemize}
\item Link capacity. The summed allocated bandwidth on this path should be no more than its capacity $c(p_\lambda)$, where $f_{B_{i,j},p_\lambda}$ is the allocated bandwidth for $B_{i,j}$ on path $p_\lambda$ and $I_{B_{i,j},p_\lambda}$ denotes whether $p_\lambda$ is selected for $B_{i,j}$.
\begin{equation}
%\begin{split}
c(p_\lambda) \geq  \displaystyle{\sum_{(s,d)\in \mathbb{A}}} \displaystyle{\sum_{B_{i,j} \in \mathbb{B}}} f_{B_{i,j},p_\lambda} \cdot I_{B_{i,j},p_\lambda}\\
%& \forall p_\lambda \in Path(s,d) \label{st:capacity}
%\end{split}
\end{equation}

\item Data size. The sum of allocated bandwidth should be no less than its size $\mathbb{S}(B_{i,j})$.
\begin{equation}
%\begin{split}
\mathbb{S}(B_{i,j}) \leq  \displaystyle{\sum_{(s,d)\in \mathbb{A}}} \displaystyle{\sum_{p_{\lambda}\in Path(s,d)}} f_{B_{i,j},p_\lambda} \cdot I_{B_{i,j},p_\lambda} \cdot \Delta T\\
%& \forall B_{i,j} \in \mathbb{B} \label{st:size}\\
%\end{split}
\end{equation}

\item Bandwidth allocation. The allocated bandwidth on path $p_\lambda$ should be the minimum of three parameters: the capacity $c(p_\lambda)$, source server upload rate $R_{up}(s)$, and destination server download rate $R_{down}(d)$.
\begin{equation}
%\begin{split}
f_{B_{i,j},p_\lambda} \leq  min \{c(p_\lambda),R_{up}(s),R_{down}(d)\}\\
%& \forall p_\lambda \in Path(s,d) \label{st:bottleneck}
%\end{split}
\end{equation}

\item Path selection. Only one path will be chosen for a particular block.
$\displaystyle{\sum_{p_\lambda \in Path(s,d)}} I_{B_{i,j},p_\lambda} = 1$
\end{packeditemize}
%\jc{maybe it's better in math after all. please write
%it in math and add comments in the end to describe the
%meaning}
%%The link capacity constraint takes effect on any arbitrary link $l_{u,v}$:
%for each link $l_{u,v}$,
%the summed allocated bandwidth on this path should be
%no more than its capacity $c(l_{u,v})$,
%and for each path $p_\lambda$, the available capacity
%$c(p_\lambda)$ should be no more than the minimum
%capacity of the consisting links.
%%The path capacity constraint takes effect on path $p_\lambda$: the available capacity $c(p_\lambda)$ should be no more than the minimum capacity of the consisting links.
%The data size constraint takes effect on blocks: the sum of allocated bandwidth should be no less than its size. The bandwidth constraint defines the allocated bandwidth for $B_{i,j}$ on $p_\lambda$: $f^*_{B_{i,j},p_{B_{i,j}}}$ should be no more than the minimum of the following three parameters: path capacity $c(p_\lambda)$, the upload rate of source node $R_{up}(s)$ and the download rate of destination node $R_{down}(d)$.

\noindent(4) {\em Objective.} To speed up the bulk data distribution, \name aims at maximizing the sum of allocated bandwidth for all the blocks over all the paths.

\begin{equation}
\centering
max \quad \displaystyle{\sum_{(s,d)\in \mathbb{A}}} \displaystyle{\sum_{B_{i,j} \in \mathbb{B}}} \displaystyle{\sum_{p_{\lambda}\in Path(s,d)}} w(B_{i,j})\cdot f_{B_{i,j},p_\lambda} \cdot I_{B_{i,j},p_\lambda}
\label{equation:objective}
\end{equation}
where $w(B_{i,j}) = \frac{pr_j}{2^{D_j-t}}$ is the weight of $B_{i,j}$, similar to \cite{zhang2015guarantee}, $pr_j$ is the priority of Task $j$, $D_j$ is the deadline and $t$ is the current time.

\mypara{Dynamic updates}
Since any change in network performance or arrival of
new requests may alter the optimal overlay
routing decisions,
%The problem can be formulated precisely according to the above definitions only when the network is static and all the conditions stay unchanged during the whole transmission period. However, it is impractical to make such assumptions because bulk data transfer and constantly changing online traffic co-exists in the network and the design choice of \name is to make dynamic bandwidth separation. Therefore,
\name updates the solutions to the above formulation every cycle of 3 seconds, which is empirically sufficient to achieve near-optimal performance (\Section\ref{subsubsec:evaluation:depth}). Unfortunately, the origin formulation is intractable in practice, because the computational overhead grows exponentially with more potential sources, paths, and data blocks.

%To make it tractable, \name decouples the problem into two steps and tries to find the optimal solution for each step.

%\jc{a missing piece is how large each block is, and why. }

\subsection{Decoupling scheduling and routing}
\label{subsec:logic:separation}

At a high level, the key insight of \name to update the overlay control problem is
to decouple the aforementioned formulation into two steps:
a {\em scheduling} step which selects a subset of blocks to be transferred
($\overrightarrow{o_\mathbb{B}}$), followed by a subsequent {\em routing}
step which determines the residual three tuples: $\langle s_{\mathbb{B}}^*, p_{\lambda}^*, f^*_{\mathbb{B},p_{\lambda}^*} \rangle$. %routing paths ($p_{\lambda}^*$) and the allocated bandwidth ($f^*_{\mathbb{B},p_{\lambda}^*}$) of these blocks.

On one hand, such decoupling significantly reduces the computational overhead of the centralized controller. As the scheduling step selects a subset of blocks and only these selected blocks will be considered in the following routing step, the searching spaces are thus significantly reduced, speeding up the execution of the centralized algorithm.

On the other hand, such decoupling is near-optimal. In the origin formulation, the optimal solution is to decide the optimal block transmission order $\overrightarrow{o_\mathbb{B}}$ for all blocks, while such decoupling converts this optimal solution into an equivalent one, i.e., to decide the optimal order to transmit the selected blocks in each scheduling cycle (so as to enforce network dynamic updates). Thus, the decoupling introduces little degradation as long as \name could find the optimal solutions for both scheduling and routing steps in each cycle.

%In the existing hybrid CDN and P2P architecture \cite{yin2009design} or centralized P2P scheme \cite{lee2003centralized}, there are also centralized servers with global information, but those servers are used for specific tasks (like system bootstrapping and maintaining global index) \cite{androutsellis2004survey}. While the centralized controller of \name faces more challenges in terms of real-time computing. The above mentioned decoupling could not only sense the changing network conditions, but also reduce the calculation scale of the centralized algorithm running on the controller.
%uses the global information to decouple scheduling and routing without introducing degradation.\jc{put the last sentence after the benefits. also, i don't get the point of this sentence.}
%, i.e., outputting the block transmission order $\overrightarrow{o}(s_i)$ ; the overlay routing procedure aims at make optimal routing strategies (assign $s_{B_{i,j}}^*$, $p_{B_{i,j}}^*$ and $f^*_{B_{i,j},p_{B_{i,j}}^*}$) for all selected blocks $B_{i,j}$ in the overlay network.
%There are two main benefits of this separation.
%\begin{packedenumerate}
%\item It reduces the computational complexity on the centralized controller side. The objective of the separated scheduling stage is to select a subset of blocks, and only these selected blocks will be routed and transferred in the following routing stage. So the separated data scheduling could avoid exploring unnecessary searching spaces for those unselected blocks.
%\item It speeds up the overall data distribution. This advantage comes from the customized block selection scheme that picks out a specific subset of blocks so as to reduce the overall completion time.
%\jc{why this is a benefit of decoupling? seems an advantage of centralized control?}
%\end{packedenumerate}
%
%\mypara{Why such decoupling is near-optimal}
%The optimal solution for the origin bulk data transfer problem is to decide the optimal order $\overrightarrow{o}(s_i)$ to transmit block for all servers, while such decoupling converts the origin problem into an equivalent one: to decide the optimal order to transmit the selected blocks in each scheduling cycle (so as to enforce network dynamic updates). Thus, the decoupling introduces no degradation as long as \name finds the optimal solutions for both of the scheduling and routing procedures.
%
%\jc{how about the following flow:
%para1: what the decoupling means; \\
%para2: why it saves costs; \\
%para3: why it could be near-optimal;\\
%para4: why people haven't thought about this or why our solution is unique}

%\jc{still unclear why this decoupling is efficient and near optimal}

%\jc{you had a table to describe all the math notations. why it's missing? it's important! im also confused what's difference between source and copy?}

\subsection{Scheduling}
\label{subsec:logic:scheduling}

The scheduling step
%tries to reduce searching space by selecting
selects the subset of blocks to be transferred in each cycle and
outputs the transmission order $\overrightarrow{o_\mathbb{B}}$. As analyzed before, the centralized overlay routing algorithm should pick the data source from $10^4$s of servers for $10^5$s of blocks, and this scale could even grow exponentially when we consider more fine-grained block partitions. Such computational complexity makes it intractable for the controller to explore all the possible paths to make optimal decisions on near-realtime timescales. Therefore, the scheduling step aims at reducing the sheer numbers of potential overlay paths, and thus reduces the searching space of the centralized algorithm.
%\jc{reducing what search space? and why we need to reduce it?}

The key to avoid introducing degradation in this process is to make sure that the optimal result is still retained in the reduced searching space.
%In other words, the optimal result is not in the space that is pruned by scheduling procedure.
%We show that the a simple-yet-efficient way of achieving optimal scheduling result is to maintain same amount of copies of each data block. We will give a full proof of the above statement in Appendix~\ref{??}, but let us  intuitively explain it here.
We claim that {\em the optimal result is yielded from the balance availability of all data blocks}, i.e., all the blocks are duplicated for the same amount of copies. Thus, a simple-yet-efficient way of avoiding degradation when reducing searching space is to balance block availability. We will give a full proof of the above statement in Appendix~\Section\ref{appendix:balance}.
%, but let us intuitively explain it here.
%thus the key concern is to balance the duplication of all blocks. So here \name prunes the searching space that will not result in balanced duplications and selects the least duplicated blocks to transmit first.
%In this way, the optimal result will be retained in the reduced searching space.
%\jc{this is a key statement! please make it more precisely}
%To be specific,
%Assume the origin data in the source DC is split into $n$ blocks, and there are $2m$ destination DCs in the WAN. Different scheduling strategies output different block subsets and lead to different intermediate transmission states, finally resulting in different completion time. Take two intermediate states as examples: 1) All of the $n$ blocks are balanced duplicated (i.e., each with $k$ duplicates); 2) Blocks are imbalanced duplicated (some have $k1$ duplicates and some have $k2$ duplicates, $(k1<k<k2)$). Let $t_1$ denote the completion time of case 1 and $t_2$ denote that of case 2. The proof in Appendix~\Section\ref{appendix:balance} shows $t_1 < t_2$, which proves the above claim.
Therefore, in this scheduling step, \name will firstly pick out the subset of blocks with the least amount of duplicates, to balance the block availability.
%\jc{wtf... this example says nothing. what's the intuition behind balancing duplicates? }

%\jc{isn't it trivial to maintain the counters?}
%For efficient selection, \name keeps a counter $c_i$ in the controller for each block and updates it once receiving finish notifications from receivers. The scheduling stage always gives priority to the smallest $c_i$. For efficient processing, \name keeps all the counters $c_i$ in a doubly linked list in an ascending order of their values. For each download, the controller selects the top item in the list (the smallest value) to be downloaded. The controller listens and serves an HTTP port, once receiving a transmission completion signal from receivers, it updates the corresponding block's counter value and adjusts its position in the linked list for further processing.

\subsection{Routing}
\label{subsec:logic:routing}

With the scheduling step selects the block set to transfer ($\overrightarrow{o_\mathbb{B}}$), the routing step then decides the residual three tuples, $\langle s_{\mathbb{B}}^*, p_{\lambda}^*, f^*_{\mathbb{B},p_{\lambda}^*} \rangle$, for all blocks.% $\mathbb{B}$, the source $s_{\mathbb{B}}^*$, the path $p_{B}^*$, and bandwidth allocation of the path $f^*_{B,p_{B}^*}$.
%In the overlay routing step, \name routes and transfers blocks according to the output of data scheduling step ($\overrightarrow{o_B}$), and then tries to make optimal routing strategy (assign $s_{B}^*$, $p_{B}^*$ and $f^*_{B,p_{B}^*}$) for each block $B$.
%Note that there are multiple potential data sources for each block in the multicast overlay network, so the objective of routing is to pick the most efficient data source and paths before allocating the bandwidths on these selected paths.

With the constraints described in \Section\ref{subsec:logic:formulation}, this formulation in Equation \ref{equation:objective} is an integer multi-commodity flow algorithm which is known to be NP-complete \cite{garg1997primal}.
%, and there is no known algorithm to find an optimal solution. 
To make this problem tractable in practice,
%we look into it from a different perspective. As the size of a task is 10s of TBs to PBs, while each block is just about several MBs,
we assume each data file can be infinitesimally split and transferred simultaneously on a set of possible paths between the source DC and the destination DC.
This approximation closely resembles \name's actual routing step as \name also splits data into tens of thousands of fine-grained data blocks (though not infinitesimally), and it can be solved efficiently by standard linear programming (LP) relaxation commonly used in the MCF problem~\cite{garg2007faster,reed2012traffic}.
The key idea is to transfer at least a fraction $\alpha$ of each task,%\jc{what's a transmission},
%Such MCF problem provides a solution to the original NP-complete problem, although with fractional flows.
and the optimal transmission rate of the relaxed problem is $F^*=\alpha\dot f^*$, where $f^*$ is the optimal transmission rate of the original routing problem.
Therefore, finding the optimal solution to the original problem becomes simply maximing $\alpha$ instead, and this can be solved in polynomial time.
%equivalent to that of the original problem $f^*$ with a relationship given by $F^* / \alpha = f^*$. Thus, the original problem is converted to maximize $\alpha$ and can be solved within polynomial time.

However, when splitting tasks infinitesimally, the number of blocks will grow considerably large and the computing time will be intolerable. \name adopts two coping strategies: on one hand, it merges the blocks with the same source and destination pair into one subtask so as to reduce the calculation scale; on the other hand, it adopts the improved fully polynomial-time approximation schemes (FPTAS) (\cite{fleischer2000approximating}) to optimize the dual problem of the origin problem and works out an $\epsilon$-optimal solution (see Appendix~\Section\ref{appendix:optimality} for the proof of near-optimality of \name).% with $\alpha' \geq \alpha_\epsilon \geq \alpha'(1-\epsilon)^{-3}$. This algorithm optimizes the dual problem of the relaxed LP problem by proceeding in phases and iterations
%\jc{sorry, but i'm totally lost. what's the key idea? is it just a standard LP relaxation?}
\jc{is this the same as ``blocks merging'' in the next section?}
