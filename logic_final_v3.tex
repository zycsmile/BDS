\section{Near-Optimal and Efficient Decision-Making Logic}
\label{sec:logic}

At the core of \name is a centralized decision-making algorithm that
periodically updates overlay routing decisions at scale in near
real-time of several seconds. \name strikes a favorable tradeoff
between solution optimality and near real-time updates by
{\em decoupling} the control logic into two steps
(\Section\ref{subsec:logic:separation}):
overlay scheduling, i.e., which data blocks to be sent
(\Section\ref{subsec:logic:scheduling}),
and routing, i.e., which paths to use to send each data block
(\Section\ref{subsec:logic:routing}), each of which
can be solved efficiently and near-optimally.% with proved guarantees
%(\Section\ref{subsec:logic:scheduling},~\ref{subsec:logic:routing}).

\subsection{Basic formulation}
\label{subsec:logic:formulation}


\begin{table}[t]
\begin{center}
\resizebox{3in}{!}{
%\begin{tabular}{p{2cm}<{\centering}|p{2cm}<{\centering}}
\begin{tabular}{| c | l|}
\hline
 \rowcolor[gray]{0.9}
\textbf{Variables} & \textbf{Meaning} \\
\hline \hline
\textit{$\mathbb{B}$} & Set of blocks of all tasks\\
\hline
\textit{$b$} & A block\\
\hline
\textit{$\rho(b)$} & The size of block $b$\\
\hline
\textit{$\mathbb{P}_{s,s'}$} & Set of paths between a source and destination pair\\
\hline
\textit{$p$} & A particular path\\
\hline
\textit{$l$} & A link on a path\\
\hline
\textit{$c(l)$} & Capacity of link $l$\\
\hline
\textit{$\Delta T$} & A scheduling cycle\\
\hline
\textit{$T_k$} & The $k$-th update cycle\\
\hline
\textit{$w^{(T_k)}_{b,s}$} & Binary: if $s$ is chosen as destination server for $b$ at $T_k$\\
\hline
\textit{$R_{up}(s)$} & Upload capacity of server $s$\\
\hline
\textit{$R_{down}(s)$} & Download capacity of server $s$\\
\hline
\textit{$f^{(T_k)}_{b,p}$} & Bandwidth allocated to send $b$ on path $p$ at $T_k$\\
%\hline
%\textit{$I^{(T_k)}_{b,p}$} & Binary: whether $p$ is selected to send $b$ at $T_k$\\
\hline
\end{tabular}
}
\end{center}
\caption{Notations used in \name's decision-making logic.}
\vspace{-0.2cm}
\label{table:para}
\end{table}

%\mypara{Under fixed network capacity}
We begin by formulating the problem of overlay traffic engineering.
Table \ref{table:para} summarizes the key variables and parameters.

The overlay traffic engineering in \name operates at a
fine granularity, both spatially and temporally. To exploit the many
overlay paths between the source and destination DCs, \name split
each data file into multiple data blocks (e.g., 2MB).
To cope with changes of network conditions and arrivals of requests,
\name updates the decisions of overlay traffic engineering every
$\Delta T$ (by default, 3 seconds\footnote{We use a fixed interval of
3 seconds, because it is long enough for \name to update decisions at
a scale of \company's workload, and short enough to adapt to typical
performance churns without noticeable impact on the completion time
of bulk data transfers.
More details in \Section\ref{sec:evaluation}}.).


Now, the problem of multicast overlay routing can be formulated as
following:

\mypara{Input} \name takes as input the following parameters:
the set of all data blocks $\mathbb{B}$, each block $b$ with size
$\rho(b)$, the set of paths from server $s'$ to $s$,
$\mathbb{P}_{s',s}$, the update cycle interval $\Delta T$, and for
each server $s$ the upload (resp. download) capacity $R_{up}(s)$
(resp. $R_{down}(s)$). Note that each path $p$ consists of several
links $l$, each defined by a pair of servers or routers. We use
$c(l)$ to denote the capacity of a link $l$.

\mypara{Output} For each cycle $T_{k}$, block $b$, server $s$, and
path $p\in\mathbb{P}_{s',s}$ destined to $s$, \name returns as output
a 2-tuple $\langle w^{(T_k)}_{b,s}, f_{b,p}^{(T_k)} \rangle$, in which $w^{(T_k)}_{b,s}$ denotes whether server $s$ is
selected as the destination server of block $b$ in $T_k$,
$f_{b,p}^{(T_k)}$ denotes how much bandwidth is allocated to send
block $b$ on path $p$ in $T_k$, and $f_{b,p}^{(T_k)}=0$ denotes
path $p$ is not selected to send block $b$ in $T_k$.

\mypara{Constraints}
\begin{packeditemize}
\item The allocated bandwidth on path $p$ must not exceed the
capacity of any link $l$ in $p$, as well as the upload capacity of
the source server $R_{up}(s)$, and the download capacity of the
destination server $R_{down}(s')$.
\begin{equation}
\begin{split}
f^{(T_k)}_{b,p} \leq min\left(min_{l\in p} c(l),
q_{b,s'}^{(T_k)}\cdot R_{up}(s'),
w_{b,s}^{(T_k)}\cdot R_{down}(s)\right) &\\
\text{for }\forall b, p\in \mathbb{P}_{s',s} &
\end{split}
\end{equation}
where $q_{b,s}^{(T_k)} = 1-\prod_{i<k} (1-w_{b,s}^{(T_i)})$ denotes
whether server $s$ has ever been selected to be the destination of
block $b$ before cycle $T_k$.

\item For all the paths, the summed allocated bandwidth of a link
should be no more than its capacity $c(l)$.
\begin{equation}
c(l) \geq \displaystyle{\sum_{b \in \mathbb{B}}}
f^{(T_k)}_{b,p},
\text{for }\forall l\in p\\
%& \forall p_\lambda \in Path(s,d) \label{st:capacity}
%\end{split}
\end{equation}

\item All blocks selected to be sent in each cycle must complete
their transfers within $\Delta T$, that is,
\begin{equation}
\begin{split}
\displaystyle{\sum_{b \in \mathbb{B}}} w_{b,s}^{(T_k)} \cdot \rho(b)
\leq \displaystyle{\sum_{p\in \mathbb{P}}}
\displaystyle{\sum_{b \in \mathbb{B}}} f^{(T_k)}_{b,p} \cdot
\Delta T,
\text{for }\forall T_k &
%& \forall B_{i,j} \in \mathbb{B} \label{st:size}\\
\end{split}
\end{equation}

%\item Only one path will be chosen for a particular block.
%\begin{equation}
%\displaystyle{\sum_{p \in \mathbb{P}}} I^{(T_k)}_{b,p} = 1, \text{for }\forall b
%\end{equation}

\item Finally, all the blocks must be transmitted at the end of all
cycles.
\begin{equation}
\displaystyle{\sum_{b\in \mathbb{B}}} \rho(b) \leq
\displaystyle{\sum_{k=1}^{N}}
\displaystyle{\sum_{p\in \mathbb{P}}}
\displaystyle{\sum_{b\in \mathbb{B}}}
f_{b,p}^{(T_k)}
\end{equation}
\end{packeditemize}


\mypara{Objective} We want to minimize the number of cycles needed to
transfer all data blocks. That is, we return as output the minimum
integer $N$ for which the above constraints have a feasible solution.

Unfortunately, this formulation is intractable in practice for two
reasons. First, it is super-linear and mixed-integer, so the
computational overhead grows exponentially with more potential source
servers, and data blocks. Second, to find the minimum integer $N$, we
need to check the feasibility of the problem for different values of
$N$.

\subsection{Decoupling scheduling and routing}
\label{subsec:logic:separation}

At a high level, the key insight behind \name is to decouple the
aforementioned formulation into two steps: a {\em scheduling} step
which selects the subset of blocks to be transferred each cycle
(i.e., $w^{(T_k)}_{b,s}$), followed by a subsequent {\em routing}
step which picks the path and allocates bandwidth to transfer the
selected blocks (i.e.,
$f_{b,p}^{(T_k)}$).

Such decoupling significantly reduces the computational overhead of
the centralized controller. As the scheduling step selects a subset
of blocks, and only these selected blocks are considered in the
subsequent routing step, the searching space is thus significantly
reduced. Mathematically, by separating the scheduling step from the
problem formulation, the routing step is reduced to a mixed-integer
LP problem, which though is not immediately tractable, can be solved
with standard techniques. Next, we present each step in more details.

\subsection{Scheduling}
\label{subsec:logic:scheduling}

The scheduling step selects the subset of blocks to be transferred in
each cycle, i.e., $w^{(T_k)}_{b,s}$.

The key to do the scheduling (pick the subset of blocks) is to make sure that the subsequent data transmission can be done in the most efficient manner. Inspired by the ``rarest-first'' strategy in BitTorrent \cite{Cohen2003Incentives} that tries to balance block availability, \name adopts a simple-yet-efficient way of selecting the data blocks: for each cycle, \name simply picks the subset of blocks with the least amount of duplicates. In other words, \name generalizes the rarest-first approach by selecting a set of blocks in each cycle, instead of a copy of a single block.

%The scheduling step selects the subset of blocks to be transferred in
%each cycle, i.e., $w^{(T_k)}_{b,s}$.
%The key to pick the right subset of blocks is to make sure that the
%optimal routing is still retained while transferring the selected data
%blocks. We prove that {\em the optimal result occurs when the
%availability of all data blocks is balanced}, i.e., all the blocks are
%duplicated with the same amount of copies (See Appendix for the proof). Thus, a
%simple-yet-efficient way of selecting the data blocks balance block
%availability: for each cycle, \name simply picks the subset of blocks
%with the least amount of duplicates. Such a scheme is similar to
%BitTorrent's ``rarest-first'' \cite{Cohen2003Incentives}, but \name
%generalizes the rarest-first approach by selecting a set of blocks in
%each cycle, instead of a single copy of data.
%\jc{I still think the proof of optimality is critical.
%Can you try put it in the appendix or at the very least, upload a
%technical report online which includes the proof.}


\subsection{Routing}
\label{subsec:logic:routing}

After the scheduling step selects the block set to transfer in each
time slot ($w^{(T_k)}_{b,s}$), the routing step decides the paths and
allocates bandwidth to transfer the selected blocks (i.e.,
$f_{b,p}^{(T_k)}$).

To minimize the transfer completion time, \name maximizes the
throughput (total data volume transferred) in each cycle $T_k$.

\begin{equation}
\centering
max \displaystyle{\sum_{p\in \mathbb{P}}}
\displaystyle{\sum_{b\in \mathbb{B}}}
f_{b,p}^{(T_k)}
%\label{equation:BDS}
\end{equation}

This is of course an approximation, since greedily maximixing the
throughput in one cycle may lead to suboptimal data availability and
lower the maximum achivable throughput in the next cycle. But in
practice, we find that this approximation can lead to significant
performance improvement over baselines, partly because the scheduling
step, described in the last section, automatically balances the
availability of blocks, so suboptimal data availability (e.g.,
starvation of blocks) caused by greedy routing decisions in past
cycles happens rarely.

This formulation, together with the constraints from
\Section\ref{subsec:logic:formulation} is essentially an integer
multi-commodity flow (MCF) algorithm, which is known to be
NP-complete \cite{garg1997primal}. To make this problem tractable in
practice, the standard approximation assumes each data file can be
infinitesimally split and transferred simultaneously on a set of
possible paths between the source and the destination. \name's actual
routing step closely resembles this approximation as \name also
splits data into tens of thousands of fine-grained data blocks
(though not infinitesimally), and it can be solved efficiently by
standard linear programming (LP) relaxation commonly used in the MCF
problem~\cite{garg2007faster,reed2012traffic}.

However, when splitting tasks infinitesimally, the number of blocks
will grow considerably large, and the computing time will be
intolerable. \name adopts two coping strategies: (1) it groups the
blocks with the same source and destination pair to reduce the
problem size (detailed in \Section\ref{subsec:system:centralized});
and (2) it uses the improved fully polynomial-time approximation
schemes (FPTAS)~\cite{fleischer2000approximating} to optimize the
dual problem of the original problem and works out an
$\epsilon$-optimal solution. These two strategies further reduces the
running time of centralized algorithm.





