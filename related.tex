\section{Related Work}

%\begin{table*}[t]
%\begin{center}
%\resizebox{\textwidth}{!}{
%\begin{tabular}{| c | c| c |c | c | c |c |}
%\hline
% \rowcolor[gray]{0.9}
%\textbf{Solutions} & \textbf{Design Goal} & \textbf{Network Structure} & \textbf{Data source} & \textbf{Global Info} & \textbf{Control} & \textbf{Hardware Modification} \\
%\hline
%\hline
%B4 \cite{jain2013b4}  & \multirow{2}{*}{Traffic engineering} & \multirow{2}{*}{Non-tiered} & \multirow{2}{*}{Determined} & \multirow{2}{*}{Yes} & \multirow{2}{*}{Software-defined}   & \multirow{2}{*}{Yes}\\
%SWAN \cite{mckeown2009software} &  &  &  &  &  & \\
%\hline
%CDN \cite{kostic2003bullet} & Spread strategy & Tiered & Determined & Yes & Centralized & No \\
%\hline
%%\multirow{2}{*}{Multicast} & No & \multirow{2}{*}{Complete} & \multirow{2}{*}{No} & \multirow{2}{*}{-} & \multirow{2}{*}{Yes}\\
%% & No & & & & \\
%%\hline
%P2P & Data sharing & Non-tiered & Undetermined & No & Distributed & No \\
%\hline
%\hline
%\textbf{\name} & Data distribution & Non-tiered & Undetermined & Yes & Centralized & No\\
%\hline
%\end{tabular}
%}
%\end{center}
%%\vspace{0.1in}
%\vspace{-0.4cm}
%\caption{ Summary of prior works. Data source means when choosing a sender, whether the sender is determined. For undetermined situations, one optimal sender is chosen from multiple candidate senders.}
%\label{table:summary}
%\end{table*}

%Efficient data distribution for IDCs is a hot research topic. Table \ref{table:summary} shows the prior works related with the data distribution problem in our paper.

Here we discuss some representative work that is related to \name in three categories.


\textbf{Overlay Network Control.}
Overlay network releases great potential to various applications, especially for data transfer applications. The representative networks include Peer-to-Peer (P2P) network and Content Delivery Network (CDN). As a style of design of distributed protocols, P2P has already been verified by many applications, such as live streaming systems (CoolStreaming \cite{zhang2005coolstreaming}, Joost \cite{Joost}, PPStream \cite{PPStream}, UUSee \cite{UUSee}), video-on-demand (VoD) applications (OceanStore \cite{oceanstore}), distributed hash tables \cite{rhea2005opendht} and even nowaday's Bitcoin \cite{eyal2016bitcoin}, but those self-organizing systems based on P2P principles suffer a long latency converging to optimal. CDN distributes services spatially relative to end-users to provide high availability and performance (e.g., to reduce page load time), serving many applications such as multimedia \cite{zhu2011multimedia} and live streaming \cite{sripanidkulchai2004analysis}.

We briefly introduce the two baselines in the evaluation section: (1) Bullet \cite{kostic2003bullet}, which enables geo-distributed nodes to self-organize into a overlay mesh. Specifically, each node uses RanSub \cite{Rodriguez2003Using} to distribute summary ticket information to other nodes and receive disjoint data from its sending peers. The main difference between \name and Bullet lies in the control scheme, i.e., \name is a centralized method that has a global view of data delivery states, while Bullet is a decentralized scheme and each node makes decision locally. (2) Akamai designs a 3-layer overlay network for delivering live streams \cite{Andreev2013Designing}, where a source forwards its streams to reflectors, and reflectors send outgoing streams to stage sinks. There are two main differences between Akamai and \name. First, Akamai adopts a 3-layer topology where edge servers receive data from their parent reflectors, while \name explores a bigger searching space without layer limitation. Second, the receiving sequence of data must be sequential in Akamai because it serves for the live streaming application. But there is no such requirements in \name, and the side effect is that \name has to decide the optimal transmission order as additional work.

%\name, is therefore designed in overlay networks with centralized control.

\textbf{Data Transfer and Rate Control.}
Rate control of transport protocols in DC-level plays an important role on data transmissions. DCTCP \cite{Alizadeh2010Data}, PDQ \cite{Hong2012Finishing}, CONGA \cite{Alizadeh2014CONGA}, DCQCN \cite{Zhu2015Congestion} and TIMELY \cite{Mittal2015TIMELY} are all classical protocols showing obvious improvements on transmission efficiency. Besides, some congestion control protocols like the credit-based ExpressPass \cite{Han2017Credit} and load balancing protocols like Hermes \cite{Zhang2017Resilient} could further reduce flow completion time by making rate control. On this basis, the recent proposed Numfabric \cite{nagaraj2016numfabric} and Domino \cite{sivaraman2016packet} further explore the potential of centralized TCP on speeding up data transfer and improving DC throughput. To some extend, co-flow scheduling \cite{Chowdhury2012Coflow,Zhang2016CODA} is a little bit similar to the multicast overlay scheduling, in terms of data parallelism. But they focus on flow-level problems while \name is designed on application-level.% \name is therefore designed in a centralized decision-making manner to approximate the optimal scheduling solution.

\textbf{Centralized Traffic Engineering.} Traffic engineering (TE) has long been a hot research topic, and many existing studies \cite{chen2012design, kavulya2010analysis, mishra2010towards, reiss2012heterogeneity, sharma2011modeling, More, zhang2011characterizing} have illustrated the challenges of scalability, heterogeneity etc., especially on inter-DC level. The representative TE systems include Google's B4 \cite{jain2013b4} and Microsoft's SWAN \cite{hong2013achieving}. B4 adopts SDN \cite{mckeown2009software} and OpenFlow \cite{OpenFlow,mckeown2008openflow} to manage individual switches and deploy customized strategies on the paths. SWAN is another online traffic engineering platform, which achieves high network utilization with its software-driven WAN. %These pair-wise solutions have substantially improved inter-DC WANs, but in inter-DC multicast overlay networks, \name is still essential.% because it can optimally schedule and route data via overlay paths of DC servers.

Overall, an application-level multicast overlay network is essential to data transfer in inter-DC WANs. Applications like user logs, search engine indexes and databases would benefit much from bulk-data multicast. Furthermore, such benefits are orthogonal to prior WAN optimizations, further improving inter-DC applications' performance.
%A pilot deployment of \name, which is in a near-optimal overlay solution over inter-DC WANs for bulk data multicast, has been deployed and evaluated in \company, achieving 3-5$\times$ speedup over the existing system and several widely used overlay routing systems.

%\textbf{Inter-DC management.} Many recent studies \cite{chen2012design, kavulya2010analysis, mishra2010towards, reiss2012heterogeneity, sharma2011modeling, More, zhang2011characterizing} have illustrated the challenges of scalability, heterogeneity and other difficulties. Under these difficulties, some companies published their inter-DC management system, for example Google's B4 \cite{jain2013b4} and Microsoft's SWAN \cite{hong2013achieving}. B4 adopts SDN \cite{mckeown2009software} and OpenFlow \cite{OpenFlow,mckeown2008openflow} to manage individual switch and deploy efficient strategies on the path. SWAN is another online traffic engineering platform, which achieves high network utilization with its software-driven WAN. But these systems can not be applied in our data distribution problem, because: 1) B4, SWAN run on their private self-control backbone networks, both of them are able to manage the links, switches and routers in a software defined manner. However, most of companies do not have the ability, such as \company, the inter-DC links are rent from Internet Service Providers (ISP) and out of self-control. Thus, all the SDN-based solutions become impractical. 2) The design goals are different. B4 and SWAN is aimed to solve the traffic engineering problem. For a particular transmission, data sources and destinations are determined, so the algorithms only need to schedule flows and make routing decisions on the links. But in our data distribution problem, data sources and destinations are undetermined. In summary, these inter-DC management platforms are impractical in our scenario.
%
%\textbf{Content Delivery Network (CDN).} The goal of CDN is to distribute services spatially relative to end-users to provide high availability and performance (e.g., to reduce page load time). Few solutions \cite{kostic2003bullet} focus on data transmission, duplicating data from origin server to servers close to users. They are limited to the tiered structure, i.e., from origin server to edge servers, and then to end servers . But in our data distribution problem, all servers can be treated as in the same tier, and this makes the data transmission more efficient and flexible.
%
%\textbf{P2P-based Schemes.} As a mature distributed application protocol, P2P has already been verified by many applications, such as live streaming systems (CoolStreaming \cite{zhang2005coolstreaming}, Joost \cite{Joost}, PPStream \cite{PPStream}, UUSee \cite{UUSee}), video-on-demand (VoD) applications (OceanStore \cite{oceanstore}), distributed hash tables \cite{rhea2005opendht} and even nowaday's Bitcoin \cite{eyal2016bitcoin}. But P2P does work in the data distribution problem because the lack of global visibility. With the only local information, P2P surely can not make optimal scheduling.
